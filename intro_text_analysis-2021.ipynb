{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis\n",
    "\n",
    "## Python Variables and Text Strings\n",
    "\n",
    "**Text String:** A text string is a set of words or characters contained in double or single quotes.\n",
    "\n",
    "**Variable:** A variable is something that holds a value that may change. In simplest terms, a variable is just a box that you can put stuff in. W are going to look at storing text strings in variables.\n",
    "\n",
    "Here is an example a text string:\n",
    "\n",
    "\"All of our technology is completely unnecessary to a happy life.\"\n",
    "\n",
    "We can assign this text string to a variable - in this case a variable called called 'quote':\n",
    "\n",
    "The benifit of storing the text string in a variable means that when we write code to manipulate the text in some way we only have to refer to the variable rather than typing the complete text.\n",
    "\n",
    "\n",
    "### First install necessary additional Python packages\n",
    "Run the following piece of code by selecting its cell and clicking the 'Run' button on the toolbar. This will only needs to be done the first time you use this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install matplotlib\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following piece of code by selecting its cell and clicking the 'Run' button on the toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = \"All of our technology is completely unnecessary to a happy life.\"\n",
    "print (quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Let's now change the case of the string.\n",
    "The following code takes the variable 'quote' and changes the text to uppercase using the upper() method. It then assigns the results to a new variable called 'big_quote'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_quote = quote.upper()\n",
    "print (big_quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it is desireable to get rid of all capitalization completely so that Python does not distinguish between, for examle, 'Office' and 'office'.\n",
    "\n",
    "### Change all words in the text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_quote = quote.lower()\n",
    "print (lower_quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining strings (concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_name = \"ada\"\n",
    "last_name = \"lovelace\"\n",
    "\n",
    "full_name = first_name + ' ' + last_name # combine strings with a space inbetween\n",
    "\n",
    "print (full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence case\n",
    "However, we would want to capitalise first names and last names when we output them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_name = full_name.title() # convert name to sentence case\n",
    "\n",
    "print(proper_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Handling\n",
    "\n",
    "### Working with a text file\n",
    "\n",
    "Download the following file to your computer (The Introduction to Origin of Species by Charles Darwin)\n",
    "\n",
    "<a href=\"https://digital-exploits.edina.ac.uk/intro-ta/files/origin-intro.txt\" download>https://digital-exploits.edina.ac.uk/intro-ta/files/origin-intro.txt</a>\n",
    "\n",
    "Right-click on the link and select 'Save Link As..'. Save the file to your Downloads directory or to another appropriate location on your computer.\n",
    "\n",
    "Once the file has been saved, go back to the Noteable home tab in the browser. \n",
    "* Select 'Upload' from the top right of the page. \n",
    "* Browse to the file.\n",
    "* Click 'Select'\n",
    "* Click on the blue 'Upload' button\n",
    "\n",
    "The file is now available to be used in Noteable (you should be able to see it in the file list under the home tab)\n",
    "\n",
    "We can open the file for reading with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"origin-intro.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the file contents to a variable\n",
    "\n",
    "Now we have opened the file we can assign the contents of the file to a variable ('txt' in this case).\n",
    "\n",
    "The following code block:\n",
    "* Assigns the text to a variable\n",
    "* Transforms all the text to lower case\n",
    "* Prints the contens of the variable to the screen\n",
    "\n",
    "Experiment with printing out differing numbers of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "txt = file.read()\n",
    "txt = txt.lower()\n",
    "print (txt[:500]) # Restrict to first 500 characters\n",
    "#print (txt[-500:]) # This would display the last 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clear this text from the console. \n",
    "\n",
    "From the top menu, select Cell > Current Outputs > Clear\n",
    "\n",
    "### Removing punctuation\n",
    "\n",
    "It will also be necessary in many cases to remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "f = open(\"origin-intro.txt\") # open file\n",
    "txt = f.read()          # add file contents to variable\n",
    "txt = txt.lower()  # lower case text\n",
    "txt = re.sub(r'[^\\w\\s]','',txt)  #remove punctuation\n",
    "print (txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "NLTK is a powerful Python package that provides a set of diverse natural languages algorithms. NLTK consists of the most common algorithms such as tokenizing, part-of-speech tagging, stemming, sentiment analysis, topic segmentation, and named entity recognition.\n",
    "\n",
    "### Tokenization\n",
    "Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentence is called Tokenization. A Token is a single entity that is building blocks for sentence or paragraph.\n",
    "\n",
    "### Sentence Tokenization\n",
    "\n",
    "Sentence tokenizen breaks text paragraph into sentences.\n",
    "\n",
    "Run the following to split the file origin-intro.txt into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "f = open(\"origin-intro.txt\") # open file\n",
    "\n",
    "txt = f.read()          # add file contents to variable\n",
    "\n",
    "tokenized_text=sent_tokenize(txt)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization\n",
    "\n",
    "Word tokenizer breaks text paragraph into words.\n",
    "\n",
    "Run the following to split the file origin-intro.txt into words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "f = open(\"origin-intro.txt\") # open file\n",
    "txt = f.read()          # add file contents to variable\n",
    "txt = re.sub(r'[^\\w\\s]','',txt)  #remove punctuation\n",
    "tokenized_text=word_tokenize(txt)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "Often when analyzing text we want to remove common words that occur multiple times but for our purposes just constitute 'noise. You can see this in the output of the code block above. NLTK can help with this by providing a ready-made list of these words (called 'stopwords') that can then be filtered out.\n",
    "\n",
    "Run the following block of code to see this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text\n",
    "\n",
    "Putting these steps together we can\n",
    "* Open a text file\n",
    "* Copy the text to a variable\n",
    "* Convert to lower case\n",
    "* Split into tokens\n",
    "* remove stopwords\n",
    "\n",
    "Run the following to see the results of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "f = open(\"origin-intro.txt\") # open file\n",
    "\n",
    "contents = f.read()          # add file contents to variable\n",
    "\n",
    "contents = contents.lower()  # lower case text\n",
    "\n",
    "contents = re.sub(r'[^\\w\\s]','',contents)  #remove punctuation\n",
    "\n",
    "tokenized_word=word_tokenize(contents)\n",
    "\n",
    "filtered_word=[]\n",
    "\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_word.append(w)\n",
    "print(\"Filterd Words:\",filtered_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count\n",
    "\n",
    "The following command will return a count of the tokens returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution\n",
    "\n",
    "The following code will generate a frequenc distribution which basically counts all the unique words in your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(filtered_word)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common words\n",
    "\n",
    "You can also generate a list of the most common words. The following code is set to output the top 10 - experiment by changing this to a different number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution Plot\n",
    "\n",
    "You can also generate a graph to provide a visual representation of frequency - again experiment by changing the number (25 in this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fdist.plot(25,cumulative=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech Tagging\n",
    "\n",
    "The primary target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word. Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.\n",
    "\n",
    "As an example we'll identify the parts of speech in the quote we used earlier (feel free to change this to an example of your own)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = \"All of our technology is completely unnecessary to a happy life\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we  split the text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "tokens=nltk.word_tokenize(quote)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we run the following code identify parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common POS Tags\n",
    "\n",
    "\n",
    "| Tag      | Part of Speech |\n",
    "| ------------- | -----|\n",
    "| DT | determiner |\n",
    "| JJ | adjective |\n",
    "| IN | preposition |\n",
    "| NN | noun singular |\n",
    "| NNS | noun plural |\n",
    "| PRP$ | possessive pronoun |\n",
    "| RB | adverb |\n",
    "| VB | verb |\n",
    "| VBZ | verb, 3rd person sing |\n",
    "\n",
    "#### For reference, a complete list of tags can be obtained by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams and common word pairs\n",
    "\n",
    "N-grams are simply all combinations of adjacent words length N that you can find in your source text.\n",
    "\n",
    "In the following example we will use the NLTK ngram function to find the most common word pairs (bi-grams).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams # function for making ngrams\n",
    "\n",
    "file = open(\"origin-intro.txt\") # open file\n",
    "txt = file.read()          # add file contents to variable\n",
    "txt = txt.lower()  # lower case text\n",
    "txt = re.sub(r'[^\\w\\s]','',txt)  #remove punctuation\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords=set(stopwords.words(\"english\"))\n",
    "\n",
    "# Apply the stopwords to the text\n",
    "txt = [word for word in txt.split() if word.lower() not in stopwords]\n",
    "txt = ' '.join(txt).lower()\n",
    "\n",
    "# get rid of punctuation\n",
    "remove = string.punctuation\n",
    "pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "text = (re.sub(pattern, \"\", txt))\n",
    "\n",
    "# first get individual words\n",
    "tokenized = text.split()\n",
    "\n",
    "# and get a list of all the bi-grams\n",
    "words = ngrams(tokenized, 2)\n",
    "\n",
    "# get the frequency of each bigram in the text\n",
    "wordsFreq = collections.Counter(words)\n",
    "\n",
    "for k,v in wordsFreq.most_common(30):\n",
    "    k = ' '.join(k)\n",
    "    print(k,v) \n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-grams\n",
    "\n",
    "Now let's look for the most common 3-word phrases:\n",
    "\n",
    "In the code above find the line:\n",
    "\n",
    "`words = ngrams(tokenized, 2)`\n",
    "\n",
    "And change it to:\n",
    "\n",
    "`words = ngrams(tokenized, 3)`\n",
    "\n",
    "Run the code again to see the results\n",
    "\n",
    "You can also change the number of ngrams returned. The line:\n",
    "\n",
    "`for k,v in wordsFreq.most_common(20):`\n",
    "\n",
    "Sets the result set to `20`. Try experimenting by changing this number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and writing CSV files \n",
    "\n",
    "We will use the following code to download a CSV and write to local CSV file.\n",
    "\n",
    "In this case the file is a CSV file containing all of Donald Trump's tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "url = 'https://learn.edina.ac.uk/intro-ta/files/trump-tweet-archive.csv' # download the file\n",
    "\n",
    "csv = urllib.request.urlopen(url).read() # assign the contents of the file to a variable (csv)\n",
    "\n",
    "with open('tweets.csv', 'wb') as file: # create a new file and save the contents of 'csv' to this file\n",
    "    file.write(csv)\n",
    "    \n",
    "    print('CSV file created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect our new file\n",
    "\n",
    "Open the file and print out the first N rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = open('tweets.csv', 'r')  # open the csv data file\n",
    "reader = csv.reader(csv_file)\n",
    "    \n",
    "cnt = 0\n",
    "for row in reader:\n",
    "    if cnt < 5:\n",
    "      print(row[-5:])\n",
    "      cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Tweets to text file\n",
    "For the next step we need to export the tweet text and write it to a text file\n",
    "\n",
    "• We will ignore the tweet id and the date\n",
    "\n",
    "• We will exclude any retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = open('tweets.csv', 'r')  # open the csv data file\n",
    "next(csv_file, None)  # skip the header row\n",
    "reader = csv.reader(csv_file)\n",
    "\n",
    "# create/open output file\n",
    "text_file = open('tweets.txt','w')\n",
    "\n",
    "\n",
    "for row in reader:\n",
    "\n",
    "    tweet = row[2]\n",
    "    if ('RT @' not in tweet): # Exclude retweets\n",
    "       text_file.write(tweet + '\\n')\n",
    "\n",
    "csv_file.close()\n",
    "text_file.close()\n",
    "\n",
    "print(\"Tweets written to 'tweets.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect our new text file\n",
    "\n",
    "### Print the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = open(\"tweets.txt\")  # open file\n",
    "\n",
    "lines = txt.readlines()\n",
    "first_lines = lines[:10]\n",
    "\n",
    "print(first_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now print the last 10 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = open(\"tweets.txt\")  # open file\n",
    "\n",
    "lines = txt.readlines()\n",
    "first_lines = lines[-10:]\n",
    "\n",
    "print(first_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "We can use the ngram function in nltk to identify the most frequent n-grams in the text file we just created.\n",
    "\n",
    "As before the number of n-grams can be changed - jus change the number in the code below where indicated in the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams  # function for making ngrams\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "source_file = open(\"tweets.txt\")  # open file\n",
    "txt = source_file.read()  # add file contents to variable\n",
    "txt = txt.lower()  # lower case text\n",
    "\n",
    "stop_words = stopwords.words('english') \n",
    "\n",
    "# Apply the stopwords to the text\n",
    "txt = [word for word in txt.split() if word.lower() not in stop_words]\n",
    "#txt = ' '.join(txt).lower()\n",
    "\n",
    "# and get a list of all the bi-grams\n",
    "pairs = ngrams(txt, 2) # Change this number to see different n-grams\n",
    "\n",
    "# get the frequency of each bigram in the text\n",
    "pairsFreq = collections.Counter(pairs)\n",
    "\n",
    "for k, v in pairsFreq.most_common(20): # Change this number to change the number of n-grams\n",
    "    k = ' '.join(k)\n",
    "    print(k, v)\n",
    "\n",
    "source_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Exercises\n",
    "\n",
    "Experiment by analysing different text files. A selection can be found on the workshop webpage (or use a file of you own choosing):\n",
    "\n",
    "[https://digital-exploits.edina.ac.uk/intro-ta](https://digital-exploits.edina.ac.uk/intro-ta/)\n",
    "\n",
    "Once the file has been saved to your computer, go back to the Noteable home tab in the browser.\n",
    "\n",
    "* Select 'Upload' from the top right of the page. \n",
    "* Browse to the file.\n",
    "* Click 'Select'\n",
    "* Click on the blue 'Upload' button\n",
    "\n",
    "The file is now available to be used in Noteable.\n",
    "\n",
    "In the code blocks replace `origin-intro.txt` with the name of your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fda35ce31702043eb00f71f47e90c41af5511afe6fb73e723507c852808de0cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
